% ==================================================================
% 3 METODOLOGIA
% ==================================================================

\chapter{METODOLOGIA}

\section{FUNDAMENTAÇÃO METODOLÓGICA DA PESQUISA}

\subsection{Natureza da Pesquisa}

Esta pesquisa caracteriza-se como um estudo quantitativo-empírico de natureza experimental em finanças. O objetivo principal é comparar, de forma cientificamente rigorosa, três estratégias fundamentais de alocação de ativos no mercado acionário brasileiro. A escolha por uma abordagem quantitativa justifica-se pela necessidade de mensurar precisamente métricas de performance financeira e estabelecer comparações objetivas entre as estratégias.

A natureza empírica do estudo baseia-se na utilização de dados reais do mercado brasileiro, contrastando com estudos puramente teóricos ou baseados em simulações. Esta escolha metodológica garante que os resultados reflitam condições reais de mercado, incluindo todas as imperfeições, assimetrias e características específicas do ambiente brasileiro.

\subsection{Paradigma Out-of-Sample}

O fundamento metodológico central desta pesquisa é a análise out-of-sample, considerada o padrão-ouro em estudos de estratégias de investimento. Esta abordagem foi popularizada por DeMiguel, Garlappi e Uppal (2009) em seu influente trabalho "Optimal Versus Naive Diversification" e tornou-se prática obrigatória em pesquisas sérias sobre alocação de ativos.

\textbf{Conceito de Out-of-Sample:} A metodologia out-of-sample divide os dados históricos em duas janelas temporais completamente separadas. A primeira janela, chamada "período de estimação" ou "in-sample", é utilizada exclusivamente para calibrar os parâmetros das estratégias (estimativas de retorno esperado, volatilidades, correlações). A segunda janela, denominada "período de teste" ou "out-of-sample", é utilizada apenas para avaliar a performance das estratégias, sem que qualquer informação deste período seja utilizada na construção das carteiras.

\textbf{Importância Científica:} Esta separação temporal rigorosa elimina o "look-ahead bias" - um dos vieses mais perniciosos em pesquisas financeiras. O look-ahead bias ocorre quando informações do futuro são inadvertidamente incorporadas na construção de estratégias, levando a resultados artificialmente otimistas que não podem ser replicados na prática. Ao garantir que nenhuma informação do período de teste seja utilizada na estimação, a metodologia out-of-sample assegura que os resultados são genuinamente preditivos.

\subsection{Divisão Temporal Específica}

Para este estudo, os dados históricos são divididos da seguinte forma:

\textbf{Janela de Estimação:} Janeiro 2016 - Dezembro 2017 (24 meses)
- Função: Calibrar todos os parâmetros necessários para as três estratégias
- Uso: Estimar retornos esperados, calcular matrizes de covariância, definir pesos iniciais
- Característica: Período relativamente tranquilo no mercado brasileiro, adequado para estimação de parâmetros base

\textbf{Janela de Teste:} Janeiro 2018 - Dezembro 2019 (24 meses)
- Função: Avaliar performance real das estratégias em condições de mercado não utilizadas na construção
- Característica: Período de alta volatilidade incluindo greve dos caminhoneiros, eleições presidenciais, e alta incerteza política e econômica
- Vantagem: Oferece teste rigoroso das estratégias em condições adversas

Esta divisão equilibrada (24 meses para cada janela) proporciona dados suficientes tanto para estimação robusta quanto para avaliação estatisticamente significativa da performance.

\subsection{Protocolo Experimental Consolidado}

A Tabela~\ref{tab:protocolo_experimental} consolida todos os aspectos metodológicos para garantir replicabilidade e eliminar questionamentos sobre o design experimental.

\begin{table}[H]
\centering
\caption{Protocolo experimental (estimação vs. teste)}
\label{tab:protocolo_experimental}
\begin{tabular}{|l|l|}
\hline
\textbf{Item} & \textbf{Especificação} \\
\hline
\multicolumn{2}{|c|}{\textbf{DESIGN TEMPORAL}} \\
\hline
Janela de estimação (L) & 24 meses (Jan 2016 - Dez 2017) \\
Janela de teste & 24 meses (Jan 2018 - Dez 2019) \\
Cadência de rebalanceamento & Semestral (janeiro e julho) \\
Frequência de dados & Mensal (fechamento do último dia útil) \\
\hline
\multicolumn{2}{|c|}{\textbf{UNIVERSO E SELEÇÃO}} \\
\hline
Seleção de ativos & Score científico: liquidez + qualidade + momentum \\
Critérios de elegibilidade & Volume > R\$ 50M/mês; Negociação > 80\% dos dias \\
Universo final & 10 ativos (BBDC4, ITUB4, PETR4, VALE3, WEGE3, etc.) \\
Controles de diversificação & Máx 40\% setor; Mín 3 setores; Sem concentração > 40\% \\
\hline
\multicolumn{2}{|c|}{\textbf{IMPLEMENTAÇÃO DAS ESTRATÉGIAS}} \\
\hline
Restrições de pesos & 0\% $\leq$ w $\leq$ 40\% por ativo; $\sum$w = 1; sem alavancagem \\
Estimativa de covariância & Amostral (principal) + Shrinkage Ledoit-Wolf (robustez) \\
Taxa livre de risco & CDI mensal (fonte: BACEN-SGS série 12) \\
Retornos esperados (MVO) & Média amostral janela de estimação \\
\hline
\multicolumn{2}{|c|}{\textbf{AVALIAÇÃO E ROBUSTEZ}} \\
\hline
Custos de transação & 10/25/50 bps por rebalance aplicados a turnover \\
Métricas primárias & Sharpe Ratio, Sortino Ratio, Maximum Drawdown \\
Testes estatísticos & Jobson-Korkie para comparação de Sharpe Ratios \\
Análises de robustez & Bootstrap (2.000 iter.), shrinkage, sensibilidade \\
\hline
\multicolumn{2}{|c|}{\textbf{CONTROLES DE VIÉS}} \\
\hline
Look-ahead bias & Janelas temporais estritamente separadas \\
Survivorship bias & Apenas ativos existentes em todo período base \\
Data-snooping & Critérios de seleção definidos ex-ante \\
P-hacking & Testes confirmam achados por múltiplas métricas \\
\hline
\end{tabular}
\footnotesize
Fonte: Elaboração própria baseada em melhores práticas (DeMiguel \textit{et al.}, 2009).
\end{table}

\textbf{Transparência Metodológica}: Este protocolo elimina ambiguidades sobre o design experimental, garantindo que a separação out-of-sample seja rigorosamente mantida e que os resultados reflitam genuína capacidade preditiva das estratégias, não ajustes a posteriori aos dados de teste.

\section{UNIVERSO DE INVESTIMENTO E SELEÇÃO DE ATIVOS}

\subsection{Fundamentação Teórica para o Número de Ativos}

A determinação do número adequado de ativos em uma carteira é uma questão fundamental em teoria de portfólio, com implicações diretas tanto para os benefícios de diversificação quanto para a complexidade de implementação.

\textbf{Evidência Clássica sobre Diversificação:} O trabalho seminal de Evans e Archer (1968) estabeleceu que carteiras contendo entre 10 a 15 ativos capturam aproximadamente 90\% dos benefícios de diversificação teoricamente possíveis. Este resultado foi obtido através de análise empírica no mercado americano e tornou-se referência fundamental na literatura.

\textbf{Adaptação para Mercados Emergentes:} No contexto de mercados emergentes, como o brasileiro, algumas considerações específicas devem ser feitas. Primeiro, estes mercados tipicamente apresentam correlações mais elevadas entre ativos individuais em comparação com mercados desenvolvidos. Segundo, a menor eficiência informacional pode gerar oportunidades de diversificação diferentes. Terceiro, a menor liquidez de alguns ativos pode limitar as opções práticas.

\textbf{Considerações Estatísticas:} Do ponto de vista estatístico, a escolha de 10 ativos mantém a razão T/N (número de observações por parâmetro estimado) em nível adequado. Com 24 meses de dados na janela de estimação e 10 ativos, temos 240 observações de retorno para estimar 10 volatilidades individuais, 45 correlações únicas, e 10 retornos esperados. Esta razão proporciona estimação estatisticamente robusta dos parâmetros necessários.

\textbf{Simplicidade Operacional:} Carteiras com número moderado de ativos facilitam implementação prática, reduzem custos de transação, e permitem monitoramento mais efetivo. Para fins acadêmicos e aplicações práticas em gestão de recursos, 10 ativos representam equilíbrio ótimo entre diversificação e operacionalidade.

\subsection{Metodologia Científica de Seleção de Ativos}

A seleção dos 10 ativos segue metodologia científica rigorosa fundamentada na literatura acadêmica, aplicada exclusivamente no período 2014-2017 para eliminação completa de look-ahead bias. Esta abordagem integra filtros de liquidez baseados em dados da Economática (Volume\$, Q Negs, Q Títs) e critérios de robustez conforme estabelecido por Amihud (2002), Roll (1984), e Jegadeesh e Titman (1993).

\subsubsection{Etapa 1: Elegibilidade Mínima}

\textbf{Completude de Dados:} Aplicação de filtro de elegibilidade baseado em Goyal e Jegadeesh (1997):
\begin{itemize}
    \item Período de avaliação: 2014-2017 (janela histórica de 4 anos)
    \item Completude mínima: $\geq$ 85\% dos dias válidos no período
    \item Ausência de interrupções superiores a 30 dias consecutivos
    \item Disponibilidade mínima: $\geq$ 22 meses válidos em 2018-2019 para teste
\end{itemize}

\subsubsection{Etapa 2: Filtros de Liquidez}

Implementação de bateria de filtros fundamentados em Roll (1984), Amihud (2002) e dados específicos da Economática:

\textbf{Volume financeiro mínimo:} Baseado em Amihud (2002)
\begin{equation}
\text{Volume}\$_{i,médio} \geq \text{R\$ 5.000.000 por dia}
\end{equation}

\textbf{Quantidade de negócios:} Proxy de atividade negocial
\begin{equation}
\text{Q Negs}_{i,médio} \geq 500 \text{ negócios por dia}
\end{equation}

\textbf{Presença em bolsa:} Continuidade de negociação
\begin{equation}
\text{Dias com Volume} > 0 \geq 90\% \text{ dos dias úteis}
\end{equation}

\textbf{Controles adicionais:} Baseados em Roll (1984) e Lo e MacKinlay (1990)
\begin{align}
ZRD_i &= \frac{\text{Dias com } r_{i,t} = 0}{\text{Total de dias negociados}} \leq 0.20 \\
\text{Autocorr}(r_{i,t}, r_{i,t-1}) &\geq -0.30
\end{align}

\subsubsection{Etapa 3: Critérios de Robustez}

Aplicação de métricas fundamentadas na literatura de fatores de risco:

\textbf{Momentum 12-1:} Conforme Jegadeesh e Titman (1993)
\begin{equation}
\text{Momentum}_{i} = \prod_{t=-12}^{-2}(1 + r_{i,t}) - 1
\end{equation}

\textbf{Volatilidade anualizada:} Baseado em Fama e French (1992)
\begin{equation}
\sigma_{i} = \sqrt{12} \times \text{std}(r_{i,t})
\end{equation}

\textbf{Maximum Drawdown:} Métrica de risco de cauda
\begin{equation}
\text{MaxDD}_i = \max_{t} \left( \frac{\text{Pico}_{t} - \text{Vale}_{t}}{\text{Pico}_{t}} \right)
\end{equation}

\textbf{Downside Deviation:} Conforme Sortino e van der Meer (1991)
\begin{equation}
\text{DD}_i = \sqrt{\frac{1}{N} \sum_{r_{i,t} < 0} r_{i,t}^2}
\end{equation}

\subsubsection{Etapa 4: Score de Seleção Composto}

Integração objetiva baseada em pesos derivados da importância relativa na literatura acadêmica. Os pesos foram estabelecidos com base na evidência empírica de poder preditivo encontrada em estudos fundamentais:

\begin{equation}
\text{Score}_i = 0.35 \times \text{Mom}_{rank} + 0.25 \times (1/\text{Vol})_{rank} + 0.20 \times (1/\text{DD})_{rank} + 0.20 \times (1/\text{Down})_{rank}
\end{equation}

onde cada componente é normalizado em percentis (0-1):
\begin{itemize}
    \item $\text{Mom}_{rank}$: Percentil do \textit{momentum} 12-1 (peso 35\%) -- Jegadeesh e Titman (1993) demonstraram que \textit{momentum} é o fator com maior poder preditivo cross-sectional de retornos em horizontes de 3-12 meses
    \item $(1/\text{Vol})_{rank}$: Percentil inverso da volatilidade (peso 25\%) -- Ang \textit{et al.} (2006) documentaram a anomalia de baixa volatilidade, onde ativos menos voláteis superam consistentemente
    \item $(1/\text{DD})_{rank}$: Percentil inverso do rebaixamento máximo (\textit{maximum drawdown}) (peso 20\%) -- Controle de risco de cauda conforme Burke (1994)
    \item $(1/\text{Down})_{rank}$: Percentil inverso do desvio \textit{downside} (peso 20\%) -- Mensuração de risco assimétrico proposta por Sortino e van der Meer (1991)
\end{itemize}

\textbf{Justificativa dos Pesos:} A ponderação 35-25-20-20 reflete a hierarquia de importância estabelecida na literatura: (1) \textit{Momentum} recebe maior peso devido à robustez cross-sectional documentada por Jegadeesh e Titman (1993) e confirmada por Fama e French (2012); (2) Volatilidade recebe segundo maior peso pela consistência da anomalia de baixa volatilidade demonstrada por Ang \textit{et al.} (2006); (3) Métricas de risco de cauda (\textit{drawdown} e \textit{downside deviation}) recebem pesos iguais menores, refletindo sua importância para controle de risco sem evidência clara de superioridade relativa entre elas.

\subsubsection{Etapa 5: Controles de Diversificação}

Aplicação de filtros finais para garantir diversificação adequada:

\textbf{Diversificação Setorial:} Restrição baseada na literatura de construção de portfólios
\begin{equation}
\max(\text{Ativos por setor}) \leq 2
\end{equation}

\textbf{Controle de Redundância Estatística:} Baseado em Markowitz (1952)
\begin{equation}
\text{Corr}(r_{i,t}, r_{j,t}) \leq 0.85 \quad \forall i \neq j
\end{equation}

\textbf{Seleção Final:} Os top 10-12 ativos por score composto, respeitando restrições de diversificação

\subsubsection{Resultado da Seleção Científica}

O processo de seleção científica analisou 50 ativos disponíveis na base Economática, aplicou os filtros rigorosos de liquidez, e resultou na seleção final de 10 ativos com alta qualidade e liquidez adequada:

\begin{center}
\textbf{LREN3, WEGE3, ABEV3, AZZA3, ALPA4, RENT3, ITUB4, ALUP11, B3SA3, BBDC4}
\end{center}

\textbf{Métricas de Liquidez dos Ativos Selecionados:}
\begin{itemize}
    \item Volume financeiro médio diário: R\$ 6M - R\$ 617M (todos > R\$ 5M critério)
    \item Quantidade de negócios médio: 756 - 43.253 negócios/dia (todos > 500 critério)
    \item Quantidade de papéis negociados: dados da coluna Q Títs da Economática
    \item Presença em bolsa: > 94\% dos dias (todos > 90\% critério)
    \item Ativos incluem blue-chips selecionados: ITUB4, BBDC4, ABEV3 (aprovados no processo científico)
\end{itemize}

\textbf{Características de Performance (2014-2017):}
\begin{itemize}
    \item Score médio de seleção: 0.583 (escala 0-1)
    \item Momentum médio 12-1: +40.7\%
    \item Volatilidade média anualizada: 27.8\%
    \item Diversificação setorial: 7 setores econômicos distintos
    \item Máximo 2 ativos por setor (diversificação garantida)
\end{itemize}

\textbf{Validação da Liquidez:} Diferentemente de estudos que utilizam seleção ad-hoc, todos os ativos selecionados atendem rigorosamente aos critérios de liquidez baseados em Volume\$ (volume financeiro), Q Negs (quantidade de negócios), Q Títs (quantidade de papéis negociados), e presença em bolsa da base Economática. Esta seleção elimina ativos com baixa liquidez que poderiam distorcer resultados empíricos.

\subsection{Eliminação Sistemática de Vieses}

O processo de seleção foi especificamente desenhado para eliminar dois tipos principais de viés que comprometem a validade de estudos empíricos em finanças:

\textbf{Survivorship Bias (Viés de Sobrevivência):} Este viés ocorre quando apenas ativos que "sobreviveram" até o final do período de análise são incluídos no estudo, ignorando aqueles que saíram de mercado por falência, delisting, ou outros motivos. No contexto deste estudo, o survivorship bias é eliminado pela seleção ex-ante dos ativos baseada exclusivamente em critérios vigentes em 31/12/2017. Importante notar que todos os 10 ativos selecionados permaneceram negociados durante todo o período de teste (2018-2019), validando ex-post a robustez da seleção.

\textbf{Look-ahead Bias (Viés de Antecipação):} Este viés, ainda mais pernicioso, ocorre quando informações do futuro são inadvertidamente utilizadas na construção de estratégias ou seleção de ativos. A prevenção deste viés é absolutamente crítica para a validade científica do estudo. Todas as decisões de seleção foram baseadas rigorosamente em informações disponíveis até o ponto de corte temporal, sem qualquer consideração de performance futura.

\subsection{Documentação e Reprodutibilidade}

Todo o processo de seleção foi documentado em formato JSON estruturado, incluindo:
- Critérios específicos aplicados
- Dados utilizados para cada critério
- Timestamp das decisões
- Lista final dos ativos selecionados com justificativas

Esta documentação garante auditabilidade completa e permite reprodução exata do processo por pesquisadores independentes, atendendo aos padrões de transparência científica.

\section{DADOS E PROCEDIMENTOS DE TRATAMENTO}

\subsection{Fonte e Qualidade dos Dados}

\textbf{Base de Dados Principal:} Todos os dados utilizados nesta pesquisa provêm da base Economática, que representa o padrão de excelência para dados financeiros brasileiros em pesquisas acadêmicas. A Economática é amplamente reconhecida pela comunidade acadêmica nacional e internacional pela qualidade, completude e rigor de suas séries históricas do mercado de capitais brasileiro.

\textbf{Vantagens da Base Economática:} A escolha desta fonte específica oferece várias vantagens críticas: (1) cobertura completa e consistente de dados históricos; (2) ajustes automáticos para eventos corporativos; (3) verificação contínua de qualidade; (4) padronização que facilita comparabilidade entre estudos; (5) rastreabilidade e auditabilidade dos dados.

\textbf{Validação da Fonte:} A confiabilidade da base Economática é atestada por seu uso em centenas de estudos acadêmicos publicados em periódicos nacionais e internacionais. Trabalhos seminais sobre o mercado brasileiro, incluindo aqueles de pesquisadores de instituições como FGV, USP, e universidades internacionais, consistentemente utilizam esta base.

\subsection{Procedimentos de Preparação dos Dados}

\textbf{Ajustes Corporativos Completos:} Uma das características mais importantes dos dados utilizados é que todas as séries de preços são previamente ajustadas pela Economática para refletir todos os eventos corporativos relevantes. Estes ajustes incluem:

- \textbf{Dividendos:} Todos os dividendos pagos são reinvestidos automaticamente, assegurando que os retornos calculados reflitam o retorno total disponível aos investidores
- \textbf{Splits e Grupamentos:} Eventos de divisão ou agrupamento de ações são ajustados retroativamente em toda a série histórica
- \textbf{Subscrições:} Direitos de subscrição são incorporados ao cálculo de retorno total
- \textbf{Juros sobre Capital Próprio:} Pagamentos de JCP são tratados como equivalentes a dividendos
- \textbf{Bonificações:} Emissões gratuitas de ações são ajustadas na série histórica

Esses ajustes são fundamentais porque garantem que os retornos calculados representem fidedignamente a experiência de um investidor real, incluindo todos os benefícios econômicos da propriedade das ações.

\textbf{Metodologia de Cálculo de Retornos:} Os retornos são calculados utilizando a metodologia padrão de log-retornos (retornos logarítmicos), expressa matematicamente como:

\begin{equation}
r_{i,t} = \ln(P_{i,t}) - \ln(P_{i,t-1}) = \ln\left(\frac{P_{i,t}}{P_{i,t-1}}\right)
\end{equation}

onde $r_{i,t}$ é o retorno do ativo $i$ no período $t$, e $P_{i,t}$ é o preço ajustado do ativo $i$ no período $t$.

\textbf{Vantagens dos Log-Retornos:} A escolha por log-retornos ao invés de retornos aritméticos oferece várias vantagens técnicas importantes:
- \textbf{Propriedade de Aditividade Temporal:} Log-retornos de múltiplos períodos podem ser somados diretamente
- \textbf{Simetria:} Tratamento matemático simétrico de ganhos e perdas
- \textbf{Aproximação Normal:} Para retornos pequenos, log-retornos aproximam-se melhor da distribuição normal
- \textbf{Facilidade de Agregação:} Simplifica cálculos de retornos de carteira e análises estatísticas

\subsection{Controle de Qualidade dos Dados}

\textbf{Verificação de Completude:} Todos os dados são sistematicamente verificados quanto à completude temporal. Para cada ativo selecionado, confirma-se a disponibilidade de preços diários para todos os dias úteis no período de análise (2016-2019). Qualquer gap nos dados é identificado e investigado.

\textbf{Consistência Temporal:} As séries são verificadas quanto à consistência temporal, garantindo que não existam saltos anômalos que não correspondam a eventos de mercado legítimos. Esta verificação inclui análise de mudanças extremas dia-a-dia que possam indicar erros de dados.

\textbf{Validação Cruzada:} Sempre que possível, dados-chave são validados através de comparação com fontes alternativas (como dados da B3 ou provedores internacionais), especialmente para eventos corporativos importantes.

\subsection{Tratamento de Outliers e Eventos Extremos}

\textbf{Identificação Sistemática de Outliers:} Retornos diários que excedem 3 desvios-padrão da média são automaticamente flagrados para investigação detalhada. Este critério, embora conservador, garante que eventos extremos legítimos não sejam erroneamente removidos.

\textbf{Processo de Investigação:} Para cada outlier identificado, realiza-se investigação para determinar sua legitimidade:
- \textbf{Consulta a Fontes de Notícias:} Verificação se o retorno extremo corresponde a notícias específicas sobre a empresa ou setor
- \textbf{Análise de Volume:} Confirmação se retornos extremos foram acompanhados por volumes de negociação elevados
- \textbf{Eventos Corporativos:} Verificação se o retorno extremo corresponde a algum evento corporativo não capturado pelos ajustes automáticos

\textbf{Critério de Manutenção:} Outliers são mantidos na base de dados se:
- Correspondem a eventos de mercado documentados
- São acompanhados por volume de negociação significativo
- Fazem sentido econômico no contexto específico

\textbf{Transparência no Tratamento:} Todos os outliers investigados e as decisões tomadas são documentados para garantir transparência e reprodutibilidade do processo.

\section{IMPLEMENTAÇÃO TÉCNICA DAS ESTRATÉGIAS}

\subsection{Estratégia de Markowitz: Fundamentação e Implementação}

\textbf{Fundamentação Teórica:} A estratégia de Markowitz, também conhecida como Mean-Variance Optimization (MVO), representa o paradigma clássico de otimização de portfólio. Desenvolvida por Harry Markowitz em 1952, esta abordagem busca encontrar a combinação de ativos que oferece o máximo retorno esperado para um dado nível de risco, ou alternativamente, o mínimo risco para um dado retorno esperado.

\textbf{Formulação Matemática:} Neste estudo, implementa-se a versão de mínima variância da otimização de Markowitz, que busca minimizar o risco da carteira sem impor restrições específicas de retorno. A formulação matemática é:

\begin{align}
\min_{w} \quad & w^T \Sigma w \label{eq:markowitz_obj} \\
\text{sujeito a:} \quad & \sum_{i=1}^{N} w_i = 1 \label{eq:markowitz_sum} \\
& w_i \geq 0 \quad \forall i = 1, ..., N \label{eq:markowitz_long}
\end{align}

onde:
- $w = [w_1, w_2, ..., w_N]^T$ é o vetor de pesos dos ativos na carteira
- $\Sigma$ é a matriz de covariância $(N \times N)$ dos retornos dos ativos
- $N = 10$ é o número de ativos na carteira

\textbf{Interpretação das Restrições:}
- Equação \ref{eq:markowitz_sum}: Garante que os pesos somem 100\%, ou seja, todo o capital é investido
- Equação \ref{eq:markowitz_long}: Impõe restrição de long-only, proibindo vendas a descoberto

\textbf{Processo de Estimação de Parâmetros:}

\textit{Estimação da Matriz de Covariância:} A matriz $\Sigma$ é estimada usando a covariância amostral dos retornos históricos na janela de estimação:

\begin{equation}
\hat{\Sigma}_{ij} = \frac{1}{T-1} \sum_{t=1}^{T} (r_{i,t} - \bar{r}_i)(r_{j,t} - \bar{r}_j)
\end{equation}

onde $T = 24$ meses é o tamanho da janela de estimação, $r_{i,t}$ é o retorno do ativo $i$ no mês $t$, e $\bar{r}_i$ é a média dos retornos do ativo $i$.

\textit{Algoritmo de Otimização:} A otimização é realizada utilizando o algoritmo SLSQP (Sequential Least Squares Programming), implementado na biblioteca scipy.optimize do Python. Este algoritmo é particularmente adequado para problemas de programação quadrática com restrições lineares e não-lineares.

\textbf{Derivação da Solução:} O problema de otimização de Markowitz é um problema de programação quadrática convexa cuja solução analítica é obtida através do método dos multiplicadores de Lagrange. A função Lagrangiana incorpora a função objetivo (minimização da variância) e as restrições de soma unitária e não-negatividade. O sistema resultante de condições de primeira ordem (condições de Karush-Kuhn-Tucker) fornece as condições necessárias e suficientes para otimalidade. Para o caso sem restrição de não-negatividade, a solução explícita é $w^* = \frac{\Sigma^{-1}\mathbf{1}}{\mathbf{1}^T\Sigma^{-1}\mathbf{1}}$, onde $\mathbf{1}$ é o vetor unitário. Na presença de restrições de desigualdade, a solução requer algoritmos de otimização numérica como SLSQP.

\textbf{Propriedades e Limitações:} A estratégia de Markowitz é altamente sensível à qualidade das estimativas de parâmetros, especialmente a matriz de covariância. Esta sensibilidade é conhecida na literatura como "sensibilidade ao erro de estimação" e representa uma das principais limitações práticas da abordagem.

\subsection{Estratégia Equal Weight: Simplicidade e Robustez}

\textbf{Fundamentação:} A estratégia Equal Weight (EW) representa o extremo oposto da sofisticação em relação ao Markowitz. Sua implementação é deliberadamente simples: todos os ativos recebem peso igual na carteira, independentemente de suas características individuais de risco e retorno.

\textbf{Formulação Matemática:} A alocação Equal Weight é definida simplesmente como:

\begin{equation}
w_i = \frac{1}{N} = \frac{1}{10} = 0.10 \quad \forall i \in \{1, 2, ..., 10\}
\end{equation}

\textbf{Vantagens Conceituais:}
- \textbf{Eliminação de Erros de Estimação:} Por não depender de estimativas de parâmetros, a estratégia EW elimina completamente erros de estimação que podem comprometer outras abordagens
- \textbf{Simplicidade Operacional:} Implementação trivial que reduz custos operacionais e possibilidade de erros
- \textbf{Robustez:} Performance consistente em diferentes condições de mercado
- \textbf{Transparência:} Facilidade de compreensão e explicação para investidores

\textbf{Fundamentação Teórica da Robustez:} A literatura acadêmica tem demonstrado que, em ambientes de alta incerteza paramétrica (como mercados emergentes), estratégias simples como Equal Weight frequentemente superam abordagens sofisticadas. Isto ocorre porque os benefícios teóricos da otimização são anulados pelos erros de estimação dos parâmetros necessários.

\textbf{Implementação Prática:} A implementação de Equal Weight requer apenas:
1. Divisão do capital total pelo número de ativos
2. Rebalanceamento periódico para manter pesos iguais
3. Nenhuma estimação de parâmetros ou otimização matemática

\subsection{Estratégia Risk Parity: Equalização de Contribuições de Risco}

\textbf{Conceito Fundamental:} A estratégia Risk Parity, também conhecida como Equal Risk Contribution (ERC), representa uma abordagem intermediária entre a simplicidade do Equal Weight e a complexidade do Markowitz. O princípio fundamental é alocar capital de forma que cada ativo contribua igualmente para o risco total da carteira.

\textbf{Definição de Contribuição de Risco:} A contribuição de risco do ativo $i$ para o risco total da carteira é definida como:

\begin{equation}
RC_i = w_i \times \frac{\partial \sigma_p}{\partial w_i} = w_i \times \frac{(\Sigma w)_i}{\sigma_p}
\end{equation}

onde:
- $RC_i$ é a contribuição de risco do ativo $i$
- $\sigma_p = \sqrt{w^T \Sigma w}$ é a volatilidade total da carteira
- $(\Sigma w)_i$ é a $i$-ésima componente do produto matriz-vetor $\Sigma w$

\textbf{Objetivo da Estratégia Risk Parity:} O objetivo é encontrar pesos $w$ tais que:

\begin{equation}
RC_i = \frac{\sigma_p}{N} \quad \forall i = 1, ..., N
\end{equation}

Isto significa que cada ativo contribui com exatamente $1/N = 10\%$ do risco total da carteira.

\textbf{Formulação como Problema de Otimização:} O problema Risk Parity pode ser formulado como um problema de otimização que minimiza a diferença entre as contribuições de risco:

\begin{align}
\min_{w} \quad & \sum_{i=1}^{N} \left(RC_i - \frac{\sigma_p}{N}\right)^2 \\
\text{sujeito a:} \quad & \sum_{i=1}^{N} w_i = 1 \\
& w_i \geq 0 \quad \forall i
\end{align}

\textbf{Algoritmo de Implementação:} A implementação utiliza algoritmo iterativo:

1. \textbf{Inicialização:} Começar com pesos iguais $w^{(0)} = (1/N, 1/N, ..., 1/N)$
2. \textbf{Cálculo de Contribuições:} Para cada iteração $k$, calcular $RC_i^{(k)}$ para todos os ativos
3. \textbf{Ajuste de Pesos:} Ajustar pesos na direção que equaliza contribuições de risco
4. \textbf{Convergência:} Parar quando $\max_i |RC_i^{(k)} - \sigma_p^{(k)}/N| < 10^{-6}$

\textbf{Vantagens da Abordagem Risk Parity:}
- \textbf{Diversificação Efetiva:} Evita concentração de risco em poucos ativos
- \textbf{Estabilidade:} Menor sensibilidade a erros de estimação que Markowitz
- \textbf{Robustez:} Utiliza apenas informações de volatilidade e correlação, mais estáveis que retornos esperados
- \textbf{Adaptação Automática:} Naturalmente reduz exposição a ativos mais voláteis

\section{METODOLOGIA OUT-OF-SAMPLE}

\subsection{Divisão Temporal}

A metodologia out-of-sample divide os dados em duas janelas:

\textbf{Janela de Estimação:} Janeiro 2016 - Dezembro 2017 (24 meses)
- Utilizada para estimar parâmetros das estratégias (médias, covariâncias)
- Calibração dos algoritmos de otimização

\textbf{Janela de Teste:} Janeiro 2018 - Dezembro 2019 (24 meses)  
- Utilizada exclusivamente para avaliação de performance
- Nenhuma informação deste período é usada na construção das estratégias

Esta divisão equilibrada proporciona dados suficientes para estimação robusta e período de teste representativo.

\subsection{Rebalanceamento}

As carteiras são rebalanceadas semestralmente (janeiro e julho) por razões práticas:

\textbf{Custos de Transação:} Frequência moderada que equilibra captura de oportunidades com custos operacionais.

\textbf{Estabilidade:} Evita over-trading que pode degradar performance líquida.

\textbf{Implementação:} Frequência típica utilizada por gestores institucionais brasileiros.

\subsection{Controle de Look-Ahead Bias}

Para garantir validade da análise out-of-sample:

1. \textbf{Seleção de Ativos:} Baseada exclusivamente em dados disponíveis até 31/12/2017
2. \textbf{Estimação de Parâmetros:} Utiliza apenas dados da janela de estimação
3. \textbf{Rebalanceamento:} Baseado apenas em informações disponíveis na data de decisão
4. \textbf{Documentação:} Processo completamente auditável e reprodutível

\section{MÉTRICAS DE AVALIAÇÃO}

As estratégias são avaliadas através de métricas padrão da literatura:

\subsection{Sharpe Ratio}

O Sharpe Ratio é calculado conforme definido no referencial teórico (vide seção sobre Índice de Sharpe), utilizando $r_f = 0,52\%$ mensal (CDI médio 2018-2019, BACEN-SGS série 12) como taxa livre de risco.

\subsection{Sortino Ratio}

O Sortino Ratio é calculado conforme definido no referencial teórico (vide seção sobre Sortino Ratio), utilizando a mesma taxa livre de risco ($r_f = 0,52\%$ mensal) como Minimum Acceptable Return (MAR).

\subsection{Maximum Drawdown}

\begin{equation}
MDD = \max_{t} \left( \frac{\text{Pico} - \text{Vale}}{\text{Pico}} \right)
\end{equation}

Representa a maior perda percentual desde um pico anterior, medindo risco de perdas extremas.

\subsection{Volatilidade Anualizada}

\begin{equation}
\sigma_{anual} = \sigma_{mensal} \times \sqrt{12}
\end{equation}

\section{TESTE DE SIGNIFICÂNCIA}

Para verificar se diferenças em Sharpe Ratios são estatisticamente significativas, utiliza-se o teste de Jobson-Korkie (1981):

\begin{equation}
t = \frac{SR_1 - SR_2}{\sqrt{\text{Var}(SR_1 - SR_2)}}
\end{equation}

\textbf{Pressupostos do Teste:} O teste de Jobson-Korkie assume que os retornos seguem distribuição normal multivariada. Embora retornos financeiros frequentemente violem este pressuposto, o teste permanece robusto para amostras de tamanho moderado ($n \geq 24$) devido ao Teorema Central do Limite. Para maior robustez, os resultados são complementados por análise de bootstrap com 1.000 iterações, que não assume normalidade e oferece estimativas não-paramétricas dos intervalos de confiança.

Este teste permite determinar se a superioridade de uma estratégia é estatisticamente robusta ou apenas resultado de acaso amostral.

\section{FERRAMENTAS COMPUTACIONAIS}

A implementação utiliza Python com as seguintes bibliotecas:

\textbf{NumPy e Pandas:} Manipulação de dados e cálculos matriciais
\textbf{SciPy:} Algoritmos de otimização (SLSQP para Markowitz, algoritmos iterativos para Risk Parity)
\textbf{Matplotlib:} Visualização de resultados

\section{LIMITAÇÕES METODOLÓGICAS}

\subsection{Limitações Reconhecidas}

\textbf{Período Específico (Limitação Central):} Este estudo analisa especificamente o período \textbf{2018-2019 (24 meses)}, que representa uma janela temporal relativamente curta para conclusões definitivas sobre eficácia de estratégias de alocação. Esta limitação temporal é particularmente relevante porque: (1) dois anos podem não capturar ciclos completos de mercado; (2) resultados podem ser específicos às condições econômicas e políticas deste período (greve dos caminhoneiros, eleições presidenciais); (3) generalizações para outros contextos temporais devem ser feitas com cautela extrema.

\textbf{Número de Ativos:} Análise limitada a 10 ativos pode não capturar toda a diversidade do mercado brasileiro.

\textbf{Custos de Transação:} Não explicitamente modelados, embora a frequência semestral de rebalanceamento minimize seu impacto.

\textbf{Estimação de Parâmetros:} Estratégias dependem de estimativas históricas que podem não refletir condições futuras.

\subsection{Validade dos Resultados}

Apesar das limitações, a metodologia out-of-sample rigorosa e o controle de vieses garantem validade científica dos resultados dentro do escopo definido.