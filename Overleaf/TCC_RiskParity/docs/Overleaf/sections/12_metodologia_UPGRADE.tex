% ==================================================================
% 3 METODOLOGIA
% ==================================================================

\chapter{METODOLOGIA}

\section{FUNDAMENTAÇÃO METODOLÓGICA DA PESQUISA}

\subsection{Natureza da Pesquisa}

Esta pesquisa caracteriza-se como um estudo empírico quantitativo que compara três estratégias de alocação de ativos no mercado acionário brasileiro. A abordagem quantitativa é utilizada para mensurar métricas de performance financeira e estabelecer comparações objetivas entre as estratégias, seguindo metodologia estabelecida na literatura acadêmica (DEMIGUEL; GARLAPPI; UPPAL, 2009).

O estudo utiliza dados reais do mercado brasileiro, seguindo a prática estabelecida em pesquisas empíricas de finanças. Esta escolha metodológica permite que os resultados reflitam condições reais de mercado, conforme recomendado por Fama e French (2012) para estudos de estratégias de investimento.

\subsection{Alinhamento Objetivos-Metodologia}

Para responder à questão central do trabalho - \textit{"qual das três estratégias de alocação apresenta melhor desempenho ajustado ao risco no mercado brasileiro durante período de alta volatilidade?"} - cada objetivo específico é atendido através de etapas metodológicas específicas:

\textbf{Objetivo 1 - Seleção de ações com critérios de liquidez:} Implementado através da metodologia de seleção de ativos (Seção 3.2), que aplica filtros rigorosos de volume financeiro, presença em bolsa e score composto, utilizando dados até 2017 para evitar look-ahead bias.

\textbf{Objetivo 2 - Implementação das três estratégias:} Realizado através da implementação técnica (Seção 3.5), onde cada estratégia é matematicamente formalizada e computacionalmente implementada: Markowitz via otimização quadrática, Equal Weight via divisão simples, e Risk Parity via algoritmo iterativo.

\textbf{Objetivo 3 - Aplicação de metodologia out-of-sample:} Executado através da divisão temporal rigorosa (Seção 3.6), separando dados de estimação (2016-2017) dos dados de teste (2018-2019), garantindo que nenhuma informação futura seja utilizada na construção das carteiras.

\textbf{Objetivo 4 - Cálculo de métricas de performance:} Implementado através das métricas de avaliação (Seção 3.7), calculando Sharpe Ratio, Sortino Ratio, volatilidade anualizada e maximum drawdown para cada estratégia no período de teste.

\textbf{Objetivo 5 - Análise de eventos específicos:} Realizado através da escolha deliberada do período 2018-2019 que inclui greve dos caminhoneiros, eleições presidenciais e alta volatilidade, permitindo observar como cada estratégia reagiu a esses choques específicos.

\textbf{Objetivo 6 - Verificação de significância estatística:} Executado através do teste de Jobson-Korkie (Seção 3.8), que compara estatisticamente os Sharpe Ratios das estratégias para determinar se diferenças observadas são estatisticamente robustas.

\textbf{Objetivo 7 - Discussão de implicações práticas:} Realizado através da análise dos resultados obtidos e discussão de sua relevância para gestores de recursos no contexto brasileiro, considerando as características específicas do mercado nacional.

\subsection{Delimitações e Escopo do Estudo}

Para garantir clareza sobre o escopo e limitações desta pesquisa, é importante estabelecer desde o início o que este estudo pretende e não pretende abordar:

\textbf{Delimitações Temporais:} Este estudo concentra-se especificamente no período 2018-2019 (24 meses) para avaliação de performance, utilizando 2016-2017 para estimação de parâmetros. Esta delimitação temporal permite análise durante período de alta volatilidade, mas os resultados devem ser interpretados considerando as características específicas deste intervalo.

\textbf{Delimitações Metodológicas:} A análise limita-se a três estratégias fundamentais de alocação (Markowitz, Equal Weight, Risk Parity), não incluindo outras abordagens como Black-Litterman, estratégias baseadas em fatores ou metodologias mais recentes de machine learning. Esta limitação permite comparação direta entre abordagens clássicas estabelecidas na literatura.

\textbf{Delimitações de Mercado:} O estudo concentra-se exclusivamente no mercado acionário brasileiro, utilizando 10 ativos selecionados por critérios de liquidez. Não aborda outros mercados, classes de ativos (renda fixa, commodities) ou carteiras globais diversificadas.

\textbf{Delimitações Práticas:} Este estudo não pretende avaliar custos reais de transação em profundidade, impacto de mercado detalhado, aspectos tributários ou restrições regulamentares específicas. O foco está na comparação teórica ajustada das estratégias em condições controladas.

\textbf{Delimitações Estatísticas:} A análise utiliza testes de significância tradicionais (Jobson-Korkie) e não emprega métodos mais avançados como bootstrap, simulações Monte Carlo ou análises de robustez extensivas, mantendo-se no escopo apropriado para estudos de graduação.

Estas delimitações demonstram consciência crítica sobre o escopo do trabalho e orientam a interpretação adequada dos resultados obtidos.

\subsection{Paradigma Out-of-Sample}

O fundamento metodológico central desta pesquisa é a análise out-of-sample, considerada o padrão-ouro em estudos de estratégias de investimento. Esta abordagem foi popularizada por DeMiguel, Garlappi e Uppal (2009) em seu influente trabalho "Optimal Versus Naive Diversification" e tornou-se prática obrigatória em pesquisas sérias sobre alocação de ativos.

\textbf{Conceito de Out-of-Sample:} A metodologia out-of-sample divide os dados históricos em duas janelas temporais completamente separadas. A primeira janela, chamada "período de estimação" ou "in-sample", é utilizada exclusivamente para calibrar os parâmetros das estratégias (estimativas de retorno esperado, volatilidades, correlações). A segunda janela, denominada "período de teste" ou "out-of-sample", é utilizada apenas para avaliar a performance das estratégias, sem que qualquer informação deste período seja utilizada na construção das carteiras.

\textbf{Importância Científica:} Esta separação temporal rigorosa elimina o "look-ahead bias" - um dos vieses mais perniciosos em pesquisas financeiras. O look-ahead bias ocorre quando informações do futuro são inadvertidamente incorporadas na construção de estratégias, levando a resultados artificialmente otimistas que não podem ser replicados na prática. Ao garantir que nenhuma informação do período de teste seja utilizada na estimação, a metodologia out-of-sample assegura que os resultados são genuinamente preditivos.

\subsection{Divisão Temporal Específica}

Para este estudo, os dados históricos são divididos da seguinte forma:

\textbf{Janela de Estimação:} Janeiro 2016 - Dezembro 2017 (24 meses)
- Função: Calibrar todos os parâmetros necessários para as três estratégias
- Uso: Estimar retornos esperados, calcular matrizes de covariância, definir pesos iniciais
- Característica: Período de relativa estabilidade, adequado para estimação de parâmetros

\textbf{Janela de Teste:} Janeiro 2018 - Dezembro 2019 (24 meses)
- Função: Avaliar performance das estratégias em condições não utilizadas na construção
- Característica: Período de alta volatilidade incluindo greve dos caminhoneiros, eleições presidenciais e alta incerteza econômica
- Importância: Permite teste das estratégias em condições de estresse de mercado

Esta divisão equilibrada segue as recomendações de DeMiguel, Garlappi e Uppal (2009), proporcionando dados suficientes para estimação e avaliação estatística da performance.

\subsection{Protocolo Experimental Consolidado}

A Tabela~\ref{tab:protocolo_experimental} consolida todos os aspectos metodológicos para garantir replicabilidade e eliminar questionamentos sobre o design experimental.

\begin{table}[H]
\centering
\caption{Protocolo experimental (estimação vs. teste)}
\label{tab:protocolo_experimental}
\begin{tabular}{|l|l|}
\hline
\textbf{Item} & \textbf{Especificação} \\
\hline
\multicolumn{2}{|c|}{\textbf{DESIGN TEMPORAL}} \\
\hline
Janela de estimação (L) & 24 meses (Jan 2016 - Dez 2017) \\
Janela de teste & 24 meses (Jan 2018 - Dez 2019) \\
Cadência de rebalanceamento & Semestral (janeiro e julho) \\
Frequência de dados & Mensal (fechamento do último dia útil) \\
\hline
\multicolumn{2}{|c|}{\textbf{UNIVERSO E SELEÇÃO}} \\
\hline
Seleção de ativos & Score composto: liquidez + qualidade + momentum \\
Critérios de elegibilidade & Volume > R\$ 5M/dia; Negociação > 90\% dos dias \\
Universo final & 10 ativos (LREN3, WEGE3, ABEV3, AZZA3, ALPA4, etc.) \\
Controles de diversificação & Máx 40\% setor; Mín 3 setores; Sem concentração > 40\% \\
\hline
\multicolumn{2}{|c|}{\textbf{IMPLEMENTAÇÃO DAS ESTRATÉGIAS}} \\
\hline
Restrições de pesos & 0\% $\leq$ w $\leq$ 40\% por ativo; $\sum$w = 1; sem alavancagem \\
Estimativa de covariância & Matriz de covariância amostral \\
Taxa livre de risco & CDI mensal (fonte: BACEN-SGS série 12) \\
Retornos esperados (MVO) & Média amostral janela de estimação \\
\hline
\multicolumn{2}{|c|}{\textbf{AVALIAÇÃO E ROBUSTEZ}} \\
\hline
Métricas primárias & Sharpe Ratio, Sortino Ratio, Maximum Drawdown \\
Testes estatísticos & Jobson-Korkie para comparação de Sharpe Ratios \\
Análises de robustez & Análise de consistência dos resultados \\
\hline
\multicolumn{2}{|c|}{\textbf{CONTROLES DE VIÉS}} \\
\hline
Look-ahead bias & Janelas temporais estritamente separadas \\
Survivorship bias & Apenas ativos existentes em todo período base \\
Data-snooping & Critérios de seleção definidos ex-ante \\
P-hacking & Testes confirmam achados por múltiplas métricas \\
\hline
\end{tabular}
\footnotesize
Fonte: Elaboração própria baseada em melhores práticas (DEMIGUEL; GARLAPPI; UPPAL, 2009).
\end{table}

\textbf{Controle Metodológico}: Este protocolo garante que a separação out-of-sample seja mantida, assegurando que os resultados reflitam a capacidade real das estratégias conforme estabelecido por DeMiguel, Garlappi e Uppal (2009).

\section{UNIVERSO DE INVESTIMENTO E SELEÇÃO DE ATIVOS}

\subsection{Fundamentação Teórica para o Número de Ativos}

A determinação do número adequado de ativos em uma carteira é uma questão fundamental em teoria de portfólio, com implicações diretas tanto para os benefícios de diversificação quanto para a complexidade de implementação.

\textbf{Evidência Clássica sobre Diversificação:} O trabalho seminal de Evans e Archer (1968) estabeleceu que carteiras contendo entre 10 a 15 ativos capturam aproximadamente 90\% dos benefícios de diversificação teoricamente possíveis. Este resultado foi obtido através de análise empírica no mercado americano e tornou-se referência fundamental na literatura.

\textbf{Adaptação para Mercados Emergentes:} No contexto de mercados emergentes, como o brasileiro, algumas considerações específicas devem ser feitas conforme estabelecido por Harvey (1995). Primeiro, estes mercados tipicamente apresentam correlações mais elevadas entre ativos individuais em comparação com mercados desenvolvidos. Segundo, a menor eficiência informacional pode gerar oportunidades de diversificação diferentes. Terceiro, a menor liquidez de alguns ativos pode limitar as opções práticas (BEKAERT; HARVEY, 2002).

\textbf{Considerações Estatísticas:} Do ponto de vista estatístico, a escolha de 10 ativos mantém a razão T/N (número de observações por parâmetro estimado) em nível adequado. Com 24 meses de dados na janela de estimação e 10 ativos, temos 240 observações de retorno para estimar 10 volatilidades individuais, 45 correlações únicas, e 10 retornos esperados. Esta razão proporciona estimação estatisticamente robusta dos parâmetros necessários.

\textbf{Simplicidade Operacional:} Carteiras com número moderado de ativos facilitam implementação prática, reduzem custos de transação, e permitem monitoramento mais efetivo. Para fins acadêmicos e aplicações práticas em gestão de recursos, 10 ativos representam equilíbrio ótimo entre diversificação e operacionalidade.

\subsection{Metodologia de Seleção de Ativos}

A seleção dos 10 ativos segue metodologia fundamentada na literatura acadêmica, aplicada exclusivamente no período 2014-2017 para evitar look-ahead bias. Esta abordagem integra filtros de liquidez baseados em dados da Economática e critérios de qualidade conforme estabelecido por Amihud (2002), Roll (1984), e Jegadeesh e Titman (1993).

\subsubsection{Etapa 1: Elegibilidade Mínima}

\textbf{Completude de Dados:} Aplicação de filtro de elegibilidade baseado em Goyal e Jegadeesh (1997):
\begin{itemize}
    \item Período de avaliação: 2014-2017 (janela histórica de 4 anos)
    \item Completude mínima: $\geq$ 85\% dos dias válidos no período
    \item Ausência de interrupções superiores a 30 dias consecutivos
    \item Disponibilidade mínima: $\geq$ 22 meses válidos em 2018-2019 para teste
\end{itemize}

\subsubsection{Etapa 2: Filtros de Liquidez}

Implementação de bateria de filtros fundamentados em Roll (1984), Amihud (2002) e dados específicos da Economática:

\textbf{Volume financeiro mínimo:} Baseado em Amihud (2002)
\begin{equation}
\text{Volume}\$_{i,\text{médio}} \geq \text{R\$ 5.000.000 por dia}
\end{equation}

\textbf{Quantidade de negócios:} Proxy de atividade negocial
\begin{equation}
\text{Q Negs}_{i,\text{médio}} \geq 500 \text{ negócios por dia}
\end{equation}

\textbf{Presença em bolsa:} Continuidade de negociação
\begin{equation}
\text{Dias com Volume} > 0 \geq 90\% \text{ dos dias úteis}
\end{equation}

\textbf{Controles adicionais:} Baseados em Roll (1984) e Lo e MacKinlay (1990)
\begin{align}
ZRD_i &= \frac{\text{Dias com } r_{i,t} = 0}{\text{Total de dias negociados}} \leq 0.20 \\
\text{Autocorr}(r_{i,t}, r_{i,t-1}) &\geq -0.30
\end{align}

\subsubsection{Etapa 3: Critérios de Robustez}

Aplicação de métricas fundamentadas na literatura de fatores de risco:

\textbf{Momentum 12-1:} Conforme Jegadeesh e Titman (1993)
\begin{equation}
\text{Momentum}_{i} = \prod_{t=-12}^{-2}(1 + r_{i,t}) - 1
\end{equation}

\textbf{Volatilidade anualizada:} Baseado em Fama e French (1992)
\begin{equation}
\sigma_{i} = \sqrt{12} \times \text{std}(r_{i,t})
\end{equation}

\textbf{Maximum Drawdown:} Métrica de risco de cauda
\begin{equation}
\text{MaxDD}_i = \max_{t} \left( \frac{\text{Pico}_{t} - \text{Vale}_{t}}{\text{Pico}_{t}} \right)
\end{equation}

\textbf{Downside Deviation:} Conforme Sortino e van der Meer (1991)
\begin{equation}
\text{DD}_i = \sqrt{\frac{1}{N} \sum_{r_{i,t} < 0} r_{i,t}^2}
\end{equation}

\subsubsection{Etapa 4: Score de Seleção Composto}

Integração objetiva baseada em pesos derivados da importância relativa na literatura acadêmica. Os pesos foram estabelecidos com base na evidência empírica de poder preditivo encontrada em estudos fundamentais:

\begin{equation}
\text{Score}_i = 0.35 \times \text{Mom}_{rank} + 0.25 \times (1/\text{Vol})_{rank} + 0.20 \times (1/\text{DD})_{rank} + 0.20 \times (1/\text{Down})_{rank}
\end{equation}

onde cada componente é normalizado em percentis (0-1):
\begin{itemize}
    \item $\text{Mom}_{rank}$: Percentil do \textit{momentum} 12-1 (peso 35\%) -- Jegadeesh e Titman (1993) demonstraram que \textit{momentum} é o fator com maior poder preditivo cross-sectional de retornos em horizontes de 3-12 meses
    \item $(1/\text{Vol})_{rank}$: Percentil inverso da volatilidade (peso 25\%) -- Ang \textit{et al.} (2006) documentaram a anomalia de baixa volatilidade, onde ativos menos voláteis superam consistentemente
    \item $(1/\text{DD})_{rank}$: Percentil inverso do rebaixamento máximo (\textit{maximum drawdown}) (peso 20\%) -- Controle de risco de cauda conforme Burke (1994)
    \item $(1/\text{Down})_{rank}$: Percentil inverso do desvio \textit{downside} (peso 20\%) -- Mensuração de risco assimétrico proposta por Sortino e van der Meer (1991)
\end{itemize}

\textbf{Justificativa dos Pesos:} A ponderação 35-25-20-20 reflete a hierarquia de importância estabelecida na literatura: (1) \textit{Momentum} recebe maior peso devido à robustez cross-sectional documentada por Jegadeesh e Titman (1993) e confirmada por Fama e French (2012); (2) Volatilidade recebe segundo maior peso pela consistência da anomalia de baixa volatilidade demonstrada por Ang \textit{et al.} (2006); (3) Métricas de risco de cauda (\textit{drawdown} e \textit{downside deviation}) recebem pesos iguais menores, refletindo sua importância para controle de risco sem evidência clara de superioridade relativa entre elas.

\subsubsection{Etapa 5: Controles de Diversificação}

Aplicação de filtros finais para garantir diversificação adequada:

\textbf{Diversificação Setorial:} Restrição baseada na literatura de construção de portfólios
\begin{equation}
\max(\text{Ativos por setor}) \leq 2
\end{equation}

\textbf{Controle de Redundância Estatística:} Baseado em Markowitz (1952)
\begin{equation}
\text{Corr}(r_{i,t}, r_{j,t}) \leq 0.85 \quad \forall i \neq j
\end{equation}

\textbf{Seleção Final:} Os top 10-12 ativos por score composto, respeitando restrições de diversificação

\subsubsection{Resultado da Seleção Científica}

O processo de seleção científica analisou 50 ativos disponíveis na base Economática, aplicou os filtros rigorosos de liquidez, e resultou na seleção final de 10 ativos com alta qualidade e liquidez adequada:

\begin{center}
\textbf{LREN3, WEGE3, ABEV3, AZZA3, ALPA4, RENT3, ITUB4, ALUP11, B3SA3, BBDC4}
\end{center}

\textbf{Métricas de Liquidez dos Ativos Selecionados:}
\begin{itemize}
    \item Volume financeiro médio diário: R\$ 6M - R\$ 617M (todos > R\$ 5M critério)
    \item Quantidade de negócios médio: 756 - 43.253 negócios/dia (todos > 500 critério)
    \item Quantidade de papéis negociados: dados da coluna Q Títs da Economática
    \item Presença em bolsa: > 94\% dos dias (todos > 90\% critério)
    \item Ativos incluem blue-chips selecionados: ITUB4, BBDC4, ABEV3 (aprovados no processo científico)
\end{itemize}

\textbf{Características de Performance (2014-2017):}
\begin{itemize}
    \item Score médio de seleção: 0.583 (escala 0-1)
    \item Momentum médio 12-1: +40.7\%
    \item Volatilidade média anualizada: 27.8\%
    \item Diversificação setorial: 7 setores econômicos distintos
    \item Máximo 2 ativos por setor (diversificação garantida)
\end{itemize}

\textbf{Validação da Liquidez:} Diferentemente de estudos que utilizam seleção ad-hoc, todos os ativos selecionados atendem rigorosamente aos critérios de liquidez baseados em Volume\$ (volume financeiro), Q Negs (quantidade de negócios), Q Títs (quantidade de papéis negociados), e presença em bolsa da base Economática. Esta seleção elimina ativos com baixa liquidez que poderiam distorcer resultados empíricos.

\subsection{Eliminação Sistemática de Vieses}

O processo de seleção foi especificamente desenhado para eliminar dois tipos principais de viés que comprometem a validade de estudos empíricos em finanças:

\textbf{Survivorship Bias (Viés de Sobrevivência):} Este viés ocorre quando apenas ativos que "sobreviveram" até o final do período de análise são incluídos no estudo, ignorando aqueles que saíram de mercado por falência, delisting, ou outros motivos. No contexto deste estudo, o survivorship bias é eliminado pela seleção ex-ante dos ativos baseada exclusivamente em critérios vigentes em 31/12/2017. Importante notar que todos os 10 ativos selecionados permaneceram negociados durante todo o período de teste (2018-2019), validando ex-post a robustez da seleção.

\textbf{Look-ahead Bias (Viés de Antecipação):} Este viés, ainda mais pernicioso, ocorre quando informações do futuro são inadvertidamente utilizadas na construção de estratégias ou seleção de ativos. A prevenção deste viés é absolutamente crítica para a validade científica do estudo. Todas as decisões de seleção foram baseadas rigorosamente em informações disponíveis até o ponto de corte temporal, sem qualquer consideração de performance futura.

\subsection{Documentação e Reprodutibilidade}

Todo o processo de seleção foi documentado, incluindo critérios aplicados, dados utilizados e lista final dos ativos selecionados com justificativas. Esta documentação permite reprodução do processo e atende aos padrões de transparência acadêmica.

\section{DADOS E PROCEDIMENTOS DE TRATAMENTO}

\subsection{Fonte e Qualidade dos Dados}

\textbf{Fonte dos Dados:} Todos os dados utilizados nesta pesquisa provêm da base Economática, amplamente utilizada em pesquisas acadêmicas sobre o mercado brasileiro conforme documentado em estudos como os de Costa Jr., Leal e Lemgruber (2000) e outros trabalhos publicados em periódicos nacionais.

\textbf{Características dos Dados:} A escolha desta fonte oferece vantagens importantes: (1) cobertura consistente de dados históricos; (2) ajustes automáticos para eventos corporativos; (3) padronização que facilita comparabilidade entre estudos; (4) qualidade verificada através de uso extensivo na literatura acadêmica brasileira.

\textbf{Uso Acadêmico:} A base Economática é utilizada consistentemente em pesquisas sobre o mercado brasileiro, sendo referência padrão para estudos empíricos em finanças no contexto nacional.

\subsection{Procedimentos de Preparação dos Dados}

\textbf{Ajustes Corporativos Completos:} Uma das características mais importantes dos dados utilizados é que todas as séries de preços são previamente ajustadas pela Economática para refletir todos os eventos corporativos relevantes. Estes ajustes incluem:

- \textbf{Dividendos:} Todos os dividendos pagos são reinvestidos automaticamente, assegurando que os retornos calculados reflitam o retorno total disponível aos investidores
- \textbf{Splits e Grupamentos:} Eventos de divisão ou agrupamento de ações são ajustados retroativamente em toda a série histórica
- \textbf{Subscrições:} Direitos de subscrição são incorporados ao cálculo de retorno total
- \textbf{Juros sobre Capital Próprio:} Pagamentos de JCP são tratados como equivalentes a dividendos
- \textbf{Bonificações:} Emissões gratuitas de ações são ajustadas na série histórica

Esses ajustes são fundamentais porque garantem que os retornos calculados representem fidedignamente a experiência de um investidor real, incluindo todos os benefícios econômicos da propriedade das ações.

\textbf{Metodologia de Cálculo de Retornos:} Os retornos são calculados utilizando a metodologia padrão de log-retornos (retornos logarítmicos), expressa matematicamente como:

\begin{equation}
r_{i,t} = \ln(P_{i,t}) - \ln(P_{i,t-1}) = \ln\left(\frac{P_{i,t}}{P_{i,t-1}}\right)
\end{equation}

onde $r_{i,t}$ é o retorno do ativo $i$ no período $t$, e $P_{i,t}$ é o preço ajustado do ativo $i$ no período $t$.

\textbf{Vantagens dos Log-Retornos:} A escolha por log-retornos ao invés de retornos aritméticos oferece várias vantagens técnicas importantes:
- \textbf{Propriedade de Aditividade Temporal:} Log-retornos de múltiplos períodos podem ser somados diretamente
- \textbf{Simetria:} Tratamento matemático simétrico de ganhos e perdas
- \textbf{Aproximação Normal:} Para retornos pequenos, log-retornos aproximam-se melhor da distribuição normal
- \textbf{Facilidade de Agregação:} Simplifica cálculos de retornos de carteira e análises estatísticas

\subsection{Controle de Qualidade dos Dados}

\textbf{Verificação de Completude:} Todos os dados são sistematicamente verificados quanto à completude temporal. Para cada ativo selecionado, confirma-se a disponibilidade de preços diários para todos os dias úteis no período de análise (2016-2019). Qualquer gap nos dados é identificado e investigado.

\textbf{Consistência Temporal:} As séries são verificadas quanto à consistência temporal, garantindo que não existam saltos anômalos que não correspondam a eventos de mercado legítimos. Esta verificação inclui análise de mudanças extremas dia-a-dia que possam indicar erros de dados.

\textbf{Validação Cruzada:} Sempre que possível, dados-chave são validados através de comparação com fontes alternativas (como dados da B3 ou provedores internacionais), especialmente para eventos corporativos importantes.

\subsection{Tratamento de Dados Extremos}

\textbf{Identificação de Outliers:} Retornos diários que excedem 3 desvios-padrão da média são verificados para determinar se correspondem a eventos de mercado legítimos ou erros de dados.

\textbf{Critério de Manutenção:} Outliers são mantidos na base quando correspondem a eventos de mercado documentados e são acompanhados por volume de negociação significativo. Este processo garante que eventos extremos reais não sejam removidos indevidamente.

\section{IMPLEMENTAÇÃO TÉCNICA DAS ESTRATÉGIAS}

\subsection{Estratégia de Markowitz: Fundamentação e Implementação}

\textbf{Fundamentação Teórica:} A estratégia de Markowitz, também conhecida como Mean-Variance Optimization (MVO), representa o paradigma clássico de otimização de portfólio. Desenvolvida por Harry Markowitz em 1952, esta abordagem busca encontrar a combinação de ativos que oferece o máximo retorno esperado para um dado nível de risco, ou alternativamente, o mínimo risco para um dado retorno esperado.

\textbf{Formulação Matemática:} Neste estudo, implementa-se a versão de mínima variância da otimização de Markowitz, que busca minimizar o risco da carteira sem impor restrições específicas de retorno. A formulação matemática é:

\begin{align}
\min_{w} \quad & w^T \Sigma w \label{eq:markowitz_obj} \\
\text{sujeito a:} \quad & \sum_{i=1}^{N} w_i = 1 \label{eq:markowitz_sum} \\
& w_i \geq 0 \quad \forall i = 1, ..., N \label{eq:markowitz_long}
\end{align}

onde:
- $w = [w_1, w_2, ..., w_N]^T$ é o vetor de pesos dos ativos na carteira
- $\Sigma$ é a matriz de covariância $(N \times N)$ dos retornos dos ativos
- $N = 10$ é o número de ativos na carteira

\textbf{Interpretação das Restrições:}
- Equação \ref{eq:markowitz_sum}: Garante que os pesos somem 100\%, ou seja, todo o capital é investido
- Equação \ref{eq:markowitz_long}: Impõe restrição de long-only, proibindo vendas a descoberto

\textbf{Processo de Estimação de Parâmetros:}

\textit{Estimação da Matriz de Covariância:} A matriz $\Sigma$ é estimada usando a covariância amostral dos retornos históricos na janela de estimação:

\begin{equation}
\hat{\Sigma}_{ij} = \frac{1}{T-1} \sum_{t=1}^{T} (r_{i,t} - \bar{r}_i)(r_{j,t} - \bar{r}_j)
\end{equation}

onde $T = 24$ meses é o tamanho da janela de estimação, $r_{i,t}$ é o retorno do ativo $i$ no mês $t$, e $\bar{r}_i$ é a média dos retornos do ativo $i$.

\textit{Algoritmo de Otimização:} A otimização é realizada utilizando o algoritmo SLSQP (Sequential Least Squares Programming), implementado na biblioteca scipy.optimize do Python. Este algoritmo é particularmente adequado para problemas de programação quadrática com restrições lineares e não-lineares.

\textbf{Solução da Otimização:} O problema de Markowitz é resolvido utilizando métodos de otimização numérica. O algoritmo busca encontrar os pesos que minimizam o risco da carteira respeitando as restrições definidas. A solução é obtida através de algoritmos computacionais implementados em Python.

\textbf{Propriedades e Limitações:} A estratégia de Markowitz é altamente sensível à qualidade das estimativas de parâmetros, especialmente a matriz de covariância. Esta sensibilidade é conhecida na literatura como "sensibilidade ao erro de estimação" e representa uma das principais limitações práticas da abordagem.

\subsection{Estratégia Equal Weight: Simplicidade e Robustez}

\textbf{Fundamentação:} A estratégia Equal Weight (EW) representa o extremo oposto da sofisticação em relação ao Markowitz. Sua implementação é deliberadamente simples: todos os ativos recebem peso igual na carteira, independentemente de suas características individuais de risco e retorno.

\textbf{Formulação Matemática:} A alocação Equal Weight é definida simplesmente como:

\begin{equation}
w_i = \frac{1}{N} = \frac{1}{10} = 0.10 \quad \forall i \in \{1, 2, ..., 10\}
\end{equation}

\textbf{Vantagens Conceituais:}
- \textbf{Eliminação de Erros de Estimação:} Por não depender de estimativas de parâmetros, a estratégia EW elimina completamente erros de estimação que podem comprometer outras abordagens
- \textbf{Simplicidade Operacional:} Implementação trivial que reduz custos operacionais e possibilidade de erros
- \textbf{Robustez:} Performance consistente em diferentes condições de mercado
- \textbf{Transparência:} Facilidade de compreensão e explicação para investidores

\textbf{Fundamentação Teórica da Robustez:} A literatura acadêmica tem demonstrado que, em ambientes de alta incerteza paramétrica (como mercados emergentes), estratégias simples como Equal Weight frequentemente superam abordagens sofisticadas. Isto ocorre porque os benefícios teóricos da otimização são anulados pelos erros de estimação dos parâmetros necessários.

\textbf{Implementação Prática:} A implementação de Equal Weight requer apenas:
1. Divisão do capital total pelo número de ativos
2. Rebalanceamento periódico para manter pesos iguais
3. Nenhuma estimação de parâmetros ou otimização matemática

\subsection{Estratégia Risk Parity: Equalização de Contribuições de Risco}

\textbf{Conceito Fundamental:} A estratégia Risk Parity, também conhecida como Equal Risk Contribution (ERC), representa uma abordagem intermediária entre a simplicidade do Equal Weight e a complexidade do Markowitz. O princípio fundamental é alocar capital de forma que cada ativo contribua igualmente para o risco total da carteira.

\textbf{Definição de Contribuição de Risco:} A contribuição de risco do ativo $i$ para o risco total da carteira é definida como:

\begin{equation}
RC_i = w_i \times \frac{\partial \sigma_p}{\partial w_i} = w_i \times \frac{(\Sigma w)_i}{\sigma_p}
\end{equation}

onde:
- $RC_i$ é a contribuição de risco do ativo $i$
- $\sigma_p = \sqrt{w^T \Sigma w}$ é a volatilidade total da carteira
- $(\Sigma w)_i$ é a $i$-ésima componente do produto matriz-vetor $\Sigma w$

\textbf{Objetivo da Estratégia Risk Parity:} O objetivo é encontrar pesos $w$ tais que:

\begin{equation}
RC_i = \frac{\sigma_p}{N} \quad \forall i = 1, ..., N
\end{equation}

Isto significa que cada ativo contribui com exatamente $1/N = 10\%$ do risco total da carteira.

\textbf{Formulação como Problema de Otimização:} O problema Risk Parity pode ser formulado como um problema de otimização que minimiza a diferença entre as contribuições de risco:

\begin{align}
\min_{w} \quad & \sum_{i=1}^{N} \left(RC_i - \frac{\sigma_p}{N}\right)^2 \\
\text{sujeito a:} \quad & \sum_{i=1}^{N} w_i = 1 \\
& w_i \geq 0 \quad \forall i
\end{align}

\textbf{Algoritmo de Implementação:} A implementação utiliza algoritmo iterativo:

1. \textbf{Inicialização:} Começar com pesos iguais $w^{(0)} = (1/N, 1/N, ..., 1/N)$
2. \textbf{Cálculo de Contribuições:} Para cada iteração $k$, calcular $RC_i^{(k)}$ para todos os ativos
3. \textbf{Ajuste de Pesos:} Ajustar pesos na direção que equaliza contribuições de risco
4. \textbf{Convergência:} Parar quando $\max_i |RC_i^{(k)} - \sigma_p^{(k)}/N| < 10^{-6}$

\textbf{Vantagens da Abordagem Risk Parity:}
- \textbf{Diversificação Efetiva:} Evita concentração de risco em poucos ativos
- \textbf{Estabilidade:} Menor sensibilidade a erros de estimação que Markowitz
- \textbf{Robustez:} Utiliza apenas informações de volatilidade e correlação, mais estáveis que retornos esperados
- \textbf{Adaptação Automática:} Naturalmente reduz exposição a ativos mais voláteis

\section{METODOLOGIA OUT-OF-SAMPLE}

\subsection{Divisão Temporal}

A metodologia out-of-sample divide os dados em duas janelas:

\textbf{Janela de Estimação:} Janeiro 2016 - Dezembro 2017 (24 meses)
- Utilizada para estimar parâmetros das estratégias (médias, covariâncias)
- Calibração dos algoritmos de otimização

\textbf{Janela de Teste:} Janeiro 2018 - Dezembro 2019 (24 meses)  
- Utilizada exclusivamente para avaliação de performance
- Nenhuma informação deste período é usada na construção das estratégias

Esta divisão equilibrada proporciona dados suficientes para estimação robusta e período de teste representativo.

\subsection{Rebalanceamento}

As carteiras são rebalanceadas semestralmente (janeiro e julho) seguindo práticas estabelecidas na literatura conforme DeMiguel, Garlappi e Uppal (2009):

\textbf{Frequência Moderada:} Equilibra captura de oportunidades com custos operacionais.

\textbf{Estabilidade:} Evita rebalanceamentos excessivos que podem afetar a performance.

\textbf{Prática Acadêmica:} Frequência comumente utilizada em estudos comparativos de estratégias de alocação.

\subsection{Controle de Look-Ahead Bias}

Para garantir validade da análise out-of-sample:

1. \textbf{Seleção de Ativos:} Baseada exclusivamente em dados disponíveis até 31/12/2017
2. \textbf{Estimação de Parâmetros:} Utiliza apenas dados da janela de estimação
3. \textbf{Rebalanceamento:} Baseado apenas em informações disponíveis na data de decisão
4. \textbf{Documentação:} Processo completamente auditável e reprodutível

\section{MÉTRICAS DE AVALIAÇÃO}

As estratégias são avaliadas através de métricas padrão da literatura:

\subsection{Sharpe Ratio}

O Sharpe Ratio é calculado utilizando a fórmula clássica:

\begin{equation}
SR = \frac{\bar{r}_p - r_f}{\sigma_p} \times \sqrt{12}
\end{equation}

onde $\bar{r}_p$ é o retorno médio mensal da carteira, $r_f = 0,52\%$ mensal (CDI médio 2018-2019, BACEN-SGS série 12) é a taxa livre de risco, $\sigma_p$ é o desvio-padrão mensal dos retornos da carteira, e $\sqrt{12}$ é o fator de anualização.

\subsection{Sortino Ratio}

O Sortino Ratio é calculado utilizando a fórmula:

\begin{equation}
SoR = \frac{\bar{r}_p - r_f}{\text{DD}_p} \times \sqrt{12}
\end{equation}

onde $\text{DD}_p$ é o downside deviation da carteira (desvio-padrão dos retornos negativos) e $r_f = 0,52\%$ mensal é utilizado como Minimum Acceptable Return (MAR).

\subsection{Maximum Drawdown}

\begin{equation}
MDD = \max_{t} \left( \frac{\text{Pico} - \text{Vale}}{\text{Pico}} \right)
\end{equation}

Representa a maior perda percentual desde um pico anterior, medindo risco de perdas extremas.

\subsection{Volatilidade Anualizada}

\begin{equation}
\sigma_{anual} = \sigma_{mensal} \times \sqrt{12}
\end{equation}

\section{TESTE DE SIGNIFICÂNCIA}

Para verificar se diferenças em Sharpe Ratios são estatisticamente significativas, utiliza-se o teste de Jobson-Korkie (1981):

\begin{equation}
t = \frac{SR_1 - SR_2}{\sqrt{\text{Var}(SR_1 - SR_2)}}
\end{equation}

\textbf{Aplicação do Teste:} O teste assume normalidade dos retornos, pressuposto que é razoavelmente atendido com a amostra de 24 meses utilizada neste estudo.

Este teste permite determinar se a superioridade de uma estratégia é estatisticamente robusta ou apenas resultado de acaso amostral.

\section{FERRAMENTAS COMPUTACIONAIS}

A implementação utiliza Python com as seguintes bibliotecas:

\textbf{NumPy e Pandas:} Manipulação de dados e cálculos matriciais
\textbf{SciPy:} Algoritmos de otimização (SLSQP para Markowitz, algoritmos iterativos para Risk Parity)
\textbf{Matplotlib:} Visualização de resultados

\section{LIMITAÇÕES METODOLÓGICAS}

\subsection{Limitações Reconhecidas}

\textbf{Período Específico (Limitação Central):} Este estudo analisa especificamente o período \textbf{2018-2019 (24 meses)}, que representa uma janela temporal relativamente curta para conclusões definitivas sobre eficácia de estratégias de alocação. Esta limitação temporal é particularmente relevante porque: (1) dois anos podem não capturar ciclos completos de mercado; (2) resultados podem ser específicos às condições econômicas e políticas deste período (greve dos caminhoneiros, eleições presidenciais); (3) generalizações para outros contextos temporais devem ser feitas com cautela extrema.

\textbf{Número de Ativos:} Análise limitada a 10 ativos pode não capturar toda a diversidade do mercado brasileiro.

\textbf{Custos de Transação:} Não explicitamente modelados, embora a frequência semestral de rebalanceamento minimize seu impacto.

\textbf{Estimação de Parâmetros:} Estratégias dependem de estimativas históricas que podem não refletir condições futuras.

\subsection{Validade dos Resultados}

Apesar das limitações, a metodologia out-of-sample rigorosa e o controle de vieses garantem validade científica dos resultados dentro do escopo definido.